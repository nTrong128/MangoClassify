{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 22:15:07.786466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-29 22:15:07.891750: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-29 22:15:07.925040: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-29 22:15:08.127709: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-29 22:15:09.819639: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 461 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define test dataset parameters (you can customize these)\n",
    "test_dataset_directory = \"../raw_to_test1/\"\n",
    "batch_size = 32  # Adjust if necessary\n",
    "img_size = (150, 150)  # Adjust to match your training image size\n",
    "\n",
    "# Load the test dataset using image_dataset_from_directory\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dataset_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=batch_size,\n",
    "    image_size=img_size,\n",
    "    shuffle=False,  # Keep it false to maintain consistency in evaluation\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your five saved models\n",
    "model_paths = [\n",
    "    # \"../model/output/mango_classifier_trained_001.keras\",\n",
    "    # \"../model/output/mango_classifier_trained4.keras\",\n",
    "    \"../model/output/mango_classifier_trained_002.keras\",\n",
    "    # \"../model/output/mango_classifier_trained5.keras\"\n",
    "    # \"../model_others/output/hau.h5\",\n",
    "    # \"../model_others/output/elk.h5\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 22:37:13.534642: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step\n",
      "\n",
      "Model_1 Evaluation Metrics:\n",
      "Precision (Unripe): 0.9124\n",
      "Precision (Ripe): 0.4709\n",
      "Precision (Rotten): 0.7630\n",
      "Recall (Unripe): 0.8117\n",
      "Recall (Ripe): 0.7120\n",
      "Recall (Rotten): 0.5659\n",
      "F1-Score (Unripe): 0.8591\n",
      "F1-Score (Ripe): 0.5669\n",
      "F1-Score (Rotten): 0.6498\n",
      "Accuracy: 0.6876\n",
      "Macro Avg Precision: 0.7154\n",
      "Macro Avg Recall: 0.6965\n",
      "Macro Avg F1-Score: 0.6919\n",
      "Weighted Avg Precision: 0.7337\n",
      "Weighted Avg Recall: 0.6876\n",
      "Weighted Avg F1-Score: 0.6973\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract true labels from the test dataset\n",
    "y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Load the models\n",
    "models = [load_model(path) for path in model_paths]\n",
    "\n",
    "# Dictionary to store the results for each model\n",
    "model_results = {}\n",
    "\n",
    "# Iterate over each model and evaluate it\n",
    "for i, model in enumerate(models):\n",
    "    # Get predictions from the model on the test dataset\n",
    "    y_pred_probs = model.predict(test_dataset)\n",
    "    \n",
    "    # Convert predicted probabilities to class labels\n",
    "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    # Calculate classification report and accuracy\n",
    "    report = classification_report(y_true, y_pred_classes, target_names=['Unripe', 'Ripe', 'Rotten'], output_dict=True)\n",
    "    accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "    \n",
    "    # Store results in the dictionary\n",
    "    model_results[f\"Model_{i+1}\"] = {\n",
    "        \"Precision (Unripe)\": report['Unripe']['precision'],\n",
    "        \"Precision (Ripe)\": report['Ripe']['precision'],\n",
    "        \"Precision (Rotten)\": report['Rotten']['precision'],\n",
    "        \"Recall (Unripe)\": report['Unripe']['recall'],\n",
    "        \"Recall (Ripe)\": report['Ripe']['recall'],\n",
    "        \"Recall (Rotten)\": report['Rotten']['recall'],\n",
    "        \"F1-Score (Unripe)\": report['Unripe']['f1-score'],\n",
    "        \"F1-Score (Ripe)\": report['Ripe']['f1-score'],\n",
    "        \"F1-Score (Rotten)\": report['Rotten']['f1-score'],\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Macro Avg Precision\": report['macro avg']['precision'],\n",
    "        \"Macro Avg Recall\": report['macro avg']['recall'],\n",
    "        \"Macro Avg F1-Score\": report['macro avg']['f1-score'],\n",
    "        \"Weighted Avg Precision\": report['weighted avg']['precision'],\n",
    "        \"Weighted Avg Recall\": report['weighted avg']['recall'],\n",
    "        \"Weighted Avg F1-Score\": report['weighted avg']['f1-score']\n",
    "    }\n",
    "\n",
    "# Print out the results for all models\n",
    "for model_name, metrics in model_results.items():\n",
    "    print(f\"\\n{model_name} Evaluation Metrics:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
